{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm\n!pip install einops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-08-15T08:13:52.395669Z","iopub.execute_input":"2023-08-15T08:13:52.396100Z","iopub.status.idle":"2023-08-15T08:14:04.421523Z","shell.execute_reply.started":"2023-08-15T08:13:52.396055Z","shell.execute_reply":"2023-08-15T08:14:04.420356Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.65.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install mmcv-full","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport matplotlib.pyplot as plt \nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms \nimport torchvision \nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:01.492054Z","iopub.execute_input":"2023-08-15T07:31:01.492428Z","iopub.status.idle":"2023-08-15T07:31:05.515257Z","shell.execute_reply.started":"2023-08-15T07:31:01.492392Z","shell.execute_reply":"2023-08-15T07:31:05.514275Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:05.516857Z","iopub.execute_input":"2023-08-15T07:31:05.517510Z","iopub.status.idle":"2023-08-15T07:31:05.552962Z","shell.execute_reply.started":"2023-08-15T07:31:05.517474Z","shell.execute_reply":"2023-08-15T07:31:05.551854Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Download Dataset","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/anminhhung/dog-cat-dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:05.555992Z","iopub.execute_input":"2023-08-15T07:31:05.556345Z","iopub.status.idle":"2023-08-15T07:31:30.857730Z","shell.execute_reply.started":"2023-08-15T07:31:05.556312Z","shell.execute_reply":"2023-08-15T07:31:30.856681Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Cloning into 'dog-cat-dataset'...\nremote: Enumerating objects: 25004, done.\u001b[K\nremote: Total 25004 (delta 0), reused 0 (delta 0), pack-reused 25004\u001b[K\nReceiving objects: 100% (25004/25004), 541.49 MiB | 28.53 MiB/s, done.\nResolving deltas: 100% (1/1), done.\nUpdating files: 100% (25001/25001), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"class DogCatDataset(Dataset):\n  def __init__(self, root_dir, transform=None):\n    self.list_images_path = []\n    self.list_labels = []\n    self.one_hot_label = {\"dogs\": 0, \"cats\": 1}\n    for sub_dir in os.listdir(root_dir):\n      path_sub_dir = os.path.join(root_dir, sub_dir)\n      for image_name in os.listdir(path_sub_dir):\n        image_path = os.path.join(path_sub_dir, image_name)\n        label = sub_dir\n        self.list_images_path.append(image_path)\n        self.list_labels.append(label)\n    \n    self.transform = transform\n  \n  def __len__(self):\n    return len(self.list_images_path)\n  \n  def __getitem__(self, idx):\n    image = cv2.imread(self.list_images_path[idx])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (224, 224))\n    image = image.astype('float')\n    label = np.array(self.one_hot_label[self.list_labels[idx]]).astype('float')\n\n    sample = (image, label)\n    if self.transform:\n      sample = self.transform(sample)\n    \n    return sample # image, label","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:30.859095Z","iopub.execute_input":"2023-08-15T07:31:30.859887Z","iopub.status.idle":"2023-08-15T07:31:30.870598Z","shell.execute_reply.started":"2023-08-15T07:31:30.859849Z","shell.execute_reply":"2023-08-15T07:31:30.869207Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class convertToTensor:\n  def __call__(self, sample):\n    image, label = sample\n\n    # opencv image: H x W x C\n    # torch tensor: C x H x W\n    image = torch.from_numpy(image).permute(2, 0, 1).float()\n    label = torch.from_numpy(label).long()\n\n    return (image, label)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:30.872352Z","iopub.execute_input":"2023-08-15T07:31:30.872771Z","iopub.status.idle":"2023-08-15T07:31:30.882665Z","shell.execute_reply.started":"2023-08-15T07:31:30.872740Z","shell.execute_reply":"2023-08-15T07:31:30.881744Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"transformed_train_data = DogCatDataset('dog-cat-dataset/data/train', transform=transforms.Compose([convertToTensor()]))\ntransformed_test_data = DogCatDataset('dog-cat-dataset/data/test', transform=transforms.Compose([convertToTensor()]))","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:30.885062Z","iopub.execute_input":"2023-08-15T07:31:30.885765Z","iopub.status.idle":"2023-08-15T07:31:30.960114Z","shell.execute_reply.started":"2023-08-15T07:31:30.885732Z","shell.execute_reply":"2023-08-15T07:31:30.959272Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data_loader = DataLoader(transformed_train_data, batch_size=32, shuffle=True)\ntest_data_loader = DataLoader(transformed_test_data, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:30.961469Z","iopub.execute_input":"2023-08-15T07:31:30.961796Z","iopub.status.idle":"2023-08-15T07:31:30.969913Z","shell.execute_reply.started":"2023-08-15T07:31:30.961765Z","shell.execute_reply":"2023-08-15T07:31:30.968883Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Helper ","metadata":{}},{"cell_type":"code","source":"'''\n    Function for computing the accuracy of the predictions over the entire data_loader\n'''\ndef get_accuracy(model, data_loader, device):\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        model.eval()\n        for images, labels in data_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return 100*(correct/total)\n\n'''\n    Function for plotting training and validation losses\n'''\ndef plot_losses(train_acc, valid_acc, train_loss, valid_loss):\n    # change the style of the plots to seaborn\n    plt.style.use('seaborn')\n\n    train_acc = np.array(train_acc)\n    valid_acc = np.array(valid_acc)\n\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n\n    ax1.plot(train_acc, color=\"blue\", label=\"Train_acc\")\n    ax1.plot(valid_acc, color=\"red\", label=\"Validation_acc\")\n    ax1.set(title=\"Acc over epochs\",\n            xlabel=\"Epoch\",\n            ylabel=\"Acc\")\n    ax1.legend()\n\n    ax2.plot(train_loss, color=\"blue\", label=\"Train_loss\")\n    ax2.plot(valid_loss, color=\"red\", label=\"Validation_loss\")\n    ax2.set(title=\"loss over epochs\",\n            xlabel=\"Epoch\",\n            ylabel=\"Loss\")\n    ax2.legend()\n\n    fig.show()\n\n    # change the plot style to default\n    plt.style.use('default')\n\n'''\n    function for the training step of the training loop\n'''\ndef train(train_loader, model, criterion, optimizer, device):\n    model.train()\n    running_loss = 0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item()\n\n        # backward and optimizer\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = running_loss / len(train_loader)\n\n    return model, optimizer, epoch_loss\n\n'''\n    function for the validation step of the training loop\n'''\ndef validate(valid_loader, model, criterion, device):\n    model.eval()\n    running_loss = 0\n\n    for images, labels in valid_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # forward pass and record loss\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        running_loss += loss.item()\n\n    epoch_loss = running_loss / len(valid_loader)\n\n    return model, epoch_loss\n\n'''\n    function defining the entire training loop\n'''\ndef training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every=1):\n    if not os.path.exists(\"save_model\"):\n      os.mkdir(\"save_model\")\n    # set object for storing metrics\n    best_loss = 1e10\n    train_losses = []\n    valid_losses = []\n    list_train_acc = []\n    list_val_acc = []\n\n    # train model\n    for epoch in range(0, epochs):\n        # training\n        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n\n        # validation\n        with torch.no_grad():\n            model, valid_loss = validate(valid_loader, model, criterion, device)\n\n        if epoch % print_every == print_every - 1:\n            train_acc = get_accuracy(model, train_loader, device=device)\n            valid_acc = get_accuracy(model, valid_loader, device=device)\n\n\n            print('Epochs: {}, Train_loss: {}, Valid_loss: {}, Train_accuracy: {}, Valid_accuracy: {}'.format(\n                    epoch, train_loss, valid_loss, train_acc, valid_acc\n                    ))\n\n            list_train_acc.append(train_acc)\n            list_val_acc.append(valid_acc)\n            train_losses.append(train_loss)\n            valid_losses.append(valid_loss)\n\n    plot_losses(list_train_acc, list_val_acc, train_losses, valid_losses)\n\n    return model, optimizer, (train_losses, valid_losses)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:30.971643Z","iopub.execute_input":"2023-08-15T07:31:30.972016Z","iopub.status.idle":"2023-08-15T07:31:30.994887Z","shell.execute_reply.started":"2023-08-15T07:31:30.971983Z","shell.execute_reply":"2023-08-15T07:31:30.994003Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Model ","metadata":{}},{"cell_type":"code","source":"import copy\nimport torch.nn as nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.layers import DropPath, trunc_normal_\nfrom timm.models.registry import register_model\nfrom timm.layers.helpers import to_2tuple\nfrom einops import rearrange\nimport torch.nn.functional as F\n\n# from mmcv.runner import load_checkpoint\n# import mmcv","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:32:55.952655Z","iopub.execute_input":"2023-08-15T09:32:55.953062Z","iopub.status.idle":"2023-08-15T09:32:55.960857Z","shell.execute_reply.started":"2023-08-15T09:32:55.953026Z","shell.execute_reply":"2023-08-15T09:32:55.959881Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 2, 'input_size': (3, 224, 224),\n        'crop_pct': .95, 'interpolation': 'bicubic',\n        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n        'classifier': 'head',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    'model_small': _cfg(crop_pct=0.9),\n    'model_medium': _cfg(crop_pct=0.95),\n}","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:32.228622Z","iopub.execute_input":"2023-08-15T07:31:32.229517Z","iopub.status.idle":"2023-08-15T07:31:32.236475Z","shell.execute_reply.started":"2023-08-15T07:31:32.229483Z","shell.execute_reply":"2023-08-15T07:31:32.235453Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## PointRecuder","metadata":{}},{"cell_type":"code","source":"class PointRecuder(nn.Module):\n    \"\"\"\n    Point Reducer is implemented by a layer of conv since it is mathmatically equal.\n    Input: tensor in shape [B, in_chans, H, W]\n    Output: tensor in shape [B, embed_dim, H/stride, W/stride]\n    \"\"\"\n\n    def __init__(self, patch_size=16, stride=16, padding=0,\n                 in_chans=3, embed_dim=768, norm_layer=None):\n        super().__init__()\n        patch_size = to_2tuple(patch_size)\n        stride = to_2tuple(stride)\n        padding = to_2tuple(padding)\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size,\n                              stride=stride, padding=padding)\n        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = self.norm(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:32.237941Z","iopub.execute_input":"2023-08-15T07:31:32.238320Z","iopub.status.idle":"2023-08-15T07:31:32.248980Z","shell.execute_reply.started":"2023-08-15T07:31:32.238286Z","shell.execute_reply":"2023-08-15T07:31:32.247937Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Norm ","metadata":{}},{"cell_type":"code","source":"class GroupNorm(nn.GroupNorm):\n    \"\"\"\n    Group Normalization with 1 group.\n    Input: tensor in shape [B, C, H, W]\n    \"\"\"\n\n    def __init__(self, num_channels, **kwargs):\n        super().__init__(1, num_channels, **kwargs)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:32.250293Z","iopub.execute_input":"2023-08-15T07:31:32.250818Z","iopub.status.idle":"2023-08-15T07:31:32.259443Z","shell.execute_reply.started":"2023-08-15T07:31:32.250784Z","shell.execute_reply":"2023-08-15T07:31:32.258229Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Pairwise_cos_sim","metadata":{}},{"cell_type":"code","source":"def pairwise_cos_sim(x1: torch.Tensor, x2: torch.Tensor):\n    \"\"\"\n    return pair-wise similarity matrix between two tensors\n    :param x1: [B,...,M,D]\n    :param x2: [B,...,N,D]\n    :return: similarity matrix [B,...,M,N]\n    \"\"\"\n    x1 = F.normalize(x1, dim=-1)\n    x2 = F.normalize(x2, dim=-1)\n\n    sim = torch.matmul(x1, x2.transpose(-2, -1))\n    return sim","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:32.260867Z","iopub.execute_input":"2023-08-15T07:31:32.261159Z","iopub.status.idle":"2023-08-15T07:31:32.274658Z","shell.execute_reply.started":"2023-08-15T07:31:32.261136Z","shell.execute_reply":"2023-08-15T07:31:32.273626Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Cluster ","metadata":{}},{"cell_type":"code","source":"class Cluster(nn.Module):\n    def __init__(self, dim, out_dim, proposal_w=2, proposal_h=2, fold_w=2, fold_h=2, heads=4, head_dim=24,\n                 return_center=False):\n        \"\"\"\n\n        :param dim:  channel nubmer\n        :param out_dim: channel nubmer\n        :param proposal_w: the sqrt(proposals) value, we can also set a different value\n        :param proposal_h: the sqrt(proposals) value, we can also set a different value\n        :param fold_w: the sqrt(number of regions) value, we can also set a different value\n        :param fold_h: the sqrt(number of regions) value, we can also set a different value\n        :param heads:  heads number in context cluster\n        :param head_dim: dimension of each head in context cluster\n        :param return_center: if just return centers instead of dispatching back (deprecated).\n        \"\"\"\n        super().__init__()\n        self.heads = heads\n        self.head_dim = head_dim\n        self.f = nn.Conv2d(dim, heads * head_dim, kernel_size=1)  # for similarity\n        self.proj = nn.Conv2d(heads * head_dim, out_dim, kernel_size=1)  # for projecting channel number\n        self.v = nn.Conv2d(dim, heads * head_dim, kernel_size=1)  # for value\n        self.sim_alpha = nn.Parameter(torch.ones(1))\n        self.sim_beta = nn.Parameter(torch.zeros(1))\n        self.centers_proposal = nn.AdaptiveAvgPool2d((proposal_w, proposal_h))\n        self.fold_w = fold_w\n        self.fold_h = fold_h\n        self.return_center = return_center\n\n    def forward(self, x):  # [b,c,w,h]\n        value = self.v(x)\n        x = self.f(x)\n        x = rearrange(x, \"b (e c) w h -> (b e) c w h\", e=self.heads)\n        value = rearrange(value, \"b (e c) w h -> (b e) c w h\", e=self.heads)\n        if self.fold_w > 1 and self.fold_h > 1:\n            # split the big feature maps to small local regions to reduce computations.\n            b0, c0, w0, h0 = x.shape\n            assert w0 % self.fold_w == 0 and h0 % self.fold_h == 0, \\\n                f\"Ensure the feature map size ({w0}*{h0}) can be divided by fold {self.fold_w}*{self.fold_h}\"\n            x = rearrange(x, \"b c (f1 w) (f2 h) -> (b f1 f2) c w h\", f1=self.fold_w,\n                          f2=self.fold_h)  # [bs*blocks,c,ks[0],ks[1]]\n            value = rearrange(value, \"b c (f1 w) (f2 h) -> (b f1 f2) c w h\", f1=self.fold_w, f2=self.fold_h)\n        b, c, w, h = x.shape\n        centers = self.centers_proposal(x)  # [b,c,C_W,C_H], we set M = C_W*C_H and N = w*h\n        value_centers = rearrange(self.centers_proposal(value), 'b c w h -> b (w h) c')  # [b,C_W,C_H,c]\n        b, c, ww, hh = centers.shape\n        sim = torch.sigmoid(\n            self.sim_beta +\n            self.sim_alpha * pairwise_cos_sim(\n                centers.reshape(b, c, -1).permute(0, 2, 1),\n                x.reshape(b, c, -1).permute(0, 2, 1)\n            )\n        )  # [B,M,N]\n        # we use mask to sololy assign each point to one center\n        sim_max, sim_max_idx = sim.max(dim=1, keepdim=True)\n        mask = torch.zeros_like(sim)  # binary #[B,M,N]\n        mask.scatter_(1, sim_max_idx, 1.)\n        sim = sim * mask\n        value2 = rearrange(value, 'b c w h -> b (w h) c')  # [B,N,D]\n        # aggregate step, out shape [B,M,D]\n        out = ((value2.unsqueeze(dim=1) * sim.unsqueeze(dim=-1)).sum(dim=2) + value_centers) / (\n                    sim.sum(dim=-1, keepdim=True) + 1.0)  # [B,M,D]\n\n        if self.return_center:\n            out = rearrange(out, \"b (w h) c -> b c w h\", w=ww)\n        else:\n            # dispatch step, return to each point in a cluster\n            out = (out.unsqueeze(dim=2) * sim.unsqueeze(dim=-1)).sum(dim=1)  # [B,N,D]\n            out = rearrange(out, \"b (w h) c -> b c w h\", w=w)\n\n        if self.fold_w > 1 and self.fold_h > 1:\n            # recover the splited regions back to big feature maps if use the region partition.\n            out = rearrange(out, \"(b f1 f2) c w h -> b c (f1 w) (f2 h)\", f1=self.fold_w, f2=self.fold_h)\n        out = rearrange(out, \"(b e) c w h -> b (e c) w h\", e=self.heads)\n        out = self.proj(out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:32.276165Z","iopub.execute_input":"2023-08-15T07:31:32.276503Z","iopub.status.idle":"2023-08-15T07:31:32.295366Z","shell.execute_reply.started":"2023-08-15T07:31:32.276472Z","shell.execute_reply":"2023-08-15T07:31:32.294434Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## MLP ","metadata":{}},{"cell_type":"code","source":"class Mlp(nn.Module):\n    \"\"\"\n    Implementation of MLP with nn.Linear (would be slightly faster in both training and inference).\n    Input: tensor with shape [B, C, H, W]\n    \"\"\"\n\n    def __init__(self, in_features, hidden_features=None,\n                 out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        x = self.fc1(x.permute(0, 2, 3, 1))\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x).permute(0, 3, 1, 2)\n        x = self.drop(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:32.296769Z","iopub.execute_input":"2023-08-15T07:31:32.297364Z","iopub.status.idle":"2023-08-15T07:31:32.309405Z","shell.execute_reply.started":"2023-08-15T07:31:32.297333Z","shell.execute_reply":"2023-08-15T07:31:32.308430Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Cluster Block","metadata":{}},{"cell_type":"code","source":"class ClusterBlock(nn.Module):\n    \"\"\"\n    Implementation of one block.\n    --dim: embedding dim\n    --mlp_ratio: mlp expansion ratio\n    --act_layer: activation\n    --norm_layer: normalization\n    --drop: dropout rate\n    --drop path: Stochastic Depth,\n        refer to https://arxiv.org/abs/1603.09382\n    --use_layer_scale, --layer_scale_init_value: LayerScale,\n        refer to https://arxiv.org/abs/2103.17239\n    \"\"\"\n\n    def __init__(self, dim, mlp_ratio=4.,\n                 act_layer=nn.GELU, norm_layer=GroupNorm,\n                 drop=0., drop_path=0.,\n                 use_layer_scale=True, layer_scale_init_value=1e-5,\n                 # for context-cluster\n                 proposal_w=2, proposal_h=2, fold_w=2, fold_h=2, heads=4, head_dim=24, return_center=False):\n\n        super().__init__()\n\n        self.norm1 = norm_layer(dim)\n        # dim, out_dim, proposal_w=2,proposal_h=2, fold_w=2, fold_h=2, heads=4, head_dim=24, return_center=False\n        self.token_mixer = Cluster(dim=dim, out_dim=dim, proposal_w=proposal_w, proposal_h=proposal_h,\n                                   fold_w=fold_w, fold_h=fold_h, heads=heads, head_dim=head_dim, return_center=False)\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer, drop=drop)\n\n        # The following two techniques are useful to train deep ContextClusters.\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.use_layer_scale = use_layer_scale\n        if use_layer_scale:\n            self.layer_scale_1 = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n            self.layer_scale_2 = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)\n\n    def forward(self, x):\n        if self.use_layer_scale:\n            x = x + self.drop_path(\n                self.layer_scale_1.unsqueeze(-1).unsqueeze(-1)\n                * self.token_mixer(self.norm1(x)))\n            x = x + self.drop_path(\n                self.layer_scale_2.unsqueeze(-1).unsqueeze(-1)\n                * self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.token_mixer(self.norm1(x)))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:32.310767Z","iopub.execute_input":"2023-08-15T07:31:32.311371Z","iopub.status.idle":"2023-08-15T07:31:32.326044Z","shell.execute_reply.started":"2023-08-15T07:31:32.311339Z","shell.execute_reply":"2023-08-15T07:31:32.324877Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Basic Block","metadata":{}},{"cell_type":"code","source":"def basic_blocks(dim, index, layers,\n                 mlp_ratio=4.,\n                 act_layer=nn.GELU, norm_layer=GroupNorm,\n                 drop_rate=.0, drop_path_rate=0.,\n                 use_layer_scale=True, layer_scale_init_value=1e-5,\n                 # for context-cluster\n                 proposal_w=2, proposal_h=2, fold_w=2, fold_h=2, heads=4, head_dim=24, return_center=False):\n    blocks = []\n    for block_idx in range(layers[index]):\n        block_dpr = drop_path_rate * ( block_idx + sum(layers[:index])) / (sum(layers) - 1)\n        blocks.append(ClusterBlock(\n            dim, mlp_ratio=mlp_ratio,\n            act_layer=act_layer, norm_layer=norm_layer,\n            drop=drop_rate, drop_path=block_dpr,\n            use_layer_scale=use_layer_scale,\n            layer_scale_init_value=layer_scale_init_value,\n            proposal_w=proposal_w, proposal_h=proposal_h, fold_w=fold_w, fold_h=fold_h,\n            heads=heads, head_dim=head_dim, return_center=False\n        ))\n    blocks = nn.Sequential(*blocks)\n\n    return blocks\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T07:31:32.327498Z","iopub.execute_input":"2023-08-15T07:31:32.327859Z","iopub.status.idle":"2023-08-15T07:31:32.345389Z","shell.execute_reply.started":"2023-08-15T07:31:32.327805Z","shell.execute_reply":"2023-08-15T07:31:32.344435Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## ContextCluster","metadata":{}},{"cell_type":"code","source":"class ContextCluster(nn.Module):\n    \"\"\"\n    ContextCluster, the main class of our model\n    --layers: [x,x,x,x], number of blocks for the 4 stages\n    --embed_dims, --mlp_ratios, the embedding dims, mlp ratios\n    --downsamples: flags to apply downsampling or not\n    --norm_layer, --act_layer: define the types of normalization and activation\n    --num_classes: number of classes for the image classification\n    --in_patch_size, --in_stride, --in_pad: specify the patch embedding\n        for the input image\n    --down_patch_size --down_stride --down_pad:\n        specify the downsample (patch embed.)\n    --fork_feat: whether output features of the 4 stages, for dense prediction\n    \"\"\"\n\n    def __init__(self, layers, embed_dims=None,\n                 mlp_ratios=None, downsamples=None,\n                 norm_layer=nn.BatchNorm2d, act_layer=nn.GELU,\n                 num_classes=1000,\n                 in_patch_size=4, in_stride=4, in_pad=0,\n                 down_patch_size=2, down_stride=2, down_pad=0,\n                 drop_rate=0., drop_path_rate=0.,\n                 use_layer_scale=True, layer_scale_init_value=1e-5,\n                 fork_feat=False,\n#                  ckpt_path=None,\n#                  pretrained=None,\n                 # the parameters for context-cluster\n                 proposal_w=[2, 2, 2, 2], proposal_h=[2, 2, 2, 2], fold_w=[8, 4, 2, 1], fold_h=[8, 4, 2, 1],\n                 heads=[2, 4, 6, 8], head_dim=[16, 16, 32, 32],\n                 **kwargs):\n\n        super().__init__()\n\n        if not fork_feat:\n            self.num_classes = num_classes\n        self.fork_feat = fork_feat\n\n        self.patch_embed = PointRecuder(\n            patch_size=in_patch_size, stride=in_stride, padding=in_pad,\n            in_chans=5, embed_dim=embed_dims[0])\n\n        # set the main block in network\n        network = []\n        for i in range(len(layers)):\n            stage = basic_blocks(embed_dims[i], i, layers,\n                                 mlp_ratio=mlp_ratios[i],\n                                 act_layer=act_layer, norm_layer=norm_layer,\n                                 drop_rate=drop_rate,\n                                 drop_path_rate=drop_path_rate,\n                                 use_layer_scale=use_layer_scale,\n                                 layer_scale_init_value=layer_scale_init_value,\n                                 proposal_w=proposal_w[i], proposal_h=proposal_h[i],\n                                 fold_w=fold_w[i], fold_h=fold_h[i], heads=heads[i], head_dim=head_dim[i],\n                                 return_center=False\n                                 )\n            network.append(stage)\n            if i >= len(layers) - 1:\n                break\n            if downsamples[i] or embed_dims[i] != embed_dims[i + 1]:\n                # downsampling between two stages\n                network.append(\n                    PointRecuder(\n                        patch_size=down_patch_size, stride=down_stride,\n                        padding=down_pad,\n                        in_chans=embed_dims[i], embed_dim=embed_dims[i + 1]\n                    )\n                )\n\n        self.network = nn.ModuleList(network)\n\n        if self.fork_feat:\n            # add a norm layer for each output\n            self.out_indices = [0, 2, 4, 6]\n            for i_emb, i_layer in enumerate(self.out_indices):\n                if i_emb == 0 and os.environ.get('FORK_LAST3', None):\n                    # TODO: more elegant way\n                    \"\"\"For RetinaNet, `start_level=1`. The first norm layer will not used.\n                    cmd: `FORK_LAST3=1 python -m torch.distributed.launch ...`\n                    \"\"\"\n                    layer = nn.Identity()\n                else:\n                    layer = norm_layer(embed_dims[i_emb])\n                layer_name = f'norm{i_layer}'\n                self.add_module(layer_name, layer)\n        else:\n            # Classifier head\n            self.norm = norm_layer(embed_dims[-1])\n            self.head = nn.Linear(\n                embed_dims[-1], num_classes) if num_classes > 0 \\\n                else nn.Identity()\n\n        self.apply(self.cls_init_weights)\n\n    # init for classification\n    def cls_init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes):\n        self.num_classes = num_classes\n        self.head = nn.Linear(\n            self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_embeddings(self, x):\n        _, c, img_w, img_h = x.shape\n        # print(f\"det img size is {img_w} * {img_h}\")\n        # register positional information buffer.\n        range_w = torch.arange(0, img_w, step=1) / (img_w - 1.0)\n        range_h = torch.arange(0, img_h, step=1) / (img_h - 1.0)\n        fea_pos = torch.stack(torch.meshgrid(range_w, range_h, indexing='ij'), dim=-1).float()\n        fea_pos = fea_pos.to(x.device)\n        fea_pos = fea_pos - 0.5\n        pos = fea_pos.permute(2, 0, 1).unsqueeze(dim=0).expand(x.shape[0], -1, -1, -1)\n        x = self.patch_embed(torch.cat([x, pos], dim=1))\n        return x\n\n    def forward_tokens(self, x):\n        outs = []\n        for idx, block in enumerate(self.network):\n            x = block(x)\n            if self.fork_feat and idx in self.out_indices:\n                norm_layer = getattr(self, f'norm{idx}')\n                x_out = norm_layer(x)\n                outs.append(x_out)\n        if self.fork_feat:\n            # output the features of four stages for dense prediction\n            return outs\n        # output only the features of last layer for image classification\n        return x\n\n    def forward(self, x):\n        # input embedding\n        x = self.forward_embeddings(x)\n        # through backbone\n        x = self.forward_tokens(x)\n        if self.fork_feat:\n            # otuput features of four stages for dense prediction\n            return x\n        x = self.norm(x)\n        cls_out = self.head(x.mean([-2, -1]))\n        # for image classification\n        return cls_out\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:39:23.708693Z","iopub.execute_input":"2023-08-15T09:39:23.709136Z","iopub.status.idle":"2023-08-15T09:39:23.737066Z","shell.execute_reply.started":"2023-08-15T09:39:23.709102Z","shell.execute_reply":"2023-08-15T09:39:23.736096Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"@register_model\ndef coc_tiny2(num_classes=1000, **kwargs):\n    layers = [3, 4, 5, 2]\n    norm_layer = GroupNorm\n    embed_dims = [32, 64, 196, 320]\n    mlp_ratios = [8, 8, 4, 4]\n    downsamples = [True, True, True, True]\n    proposal_w = [4, 2, 7, 4]\n    proposal_h = [4, 2, 7, 4]\n    fold_w = [7, 7, 1, 1]\n    fold_h = [7, 7, 1, 1]\n    heads = [4, 4, 8, 8]\n    head_dim = [24, 24, 24, 24]\n    down_patch_size = 3\n    down_pad = 1\n    model = ContextCluster(\n        layers, embed_dims=embed_dims, norm_layer=norm_layer, num_classes=num_classes,\n        mlp_ratios=mlp_ratios, downsamples=downsamples,\n        down_patch_size=down_patch_size, down_pad=down_pad,\n        proposal_w=proposal_w, proposal_h=proposal_h, fold_w=fold_w, fold_h=fold_h,\n        heads=heads, head_dim=head_dim,\n        **kwargs)\n    model.default_cfg = default_cfgs['model_small']\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:39:36.095827Z","iopub.execute_input":"2023-08-15T09:39:36.096214Z","iopub.status.idle":"2023-08-15T09:39:36.104937Z","shell.execute_reply.started":"2023-08-15T09:39:36.096183Z","shell.execute_reply":"2023-08-15T09:39:36.103859Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# Train ","metadata":{}},{"cell_type":"code","source":"# gdown pretrain \n!gdown --id 1TNDLpESESQSVmWXKS-EebZUl2CKNH6a2","metadata":{"execution":{"iopub.status.busy":"2023-08-15T08:14:28.707078Z","iopub.execute_input":"2023-08-15T08:14:28.707471Z","iopub.status.idle":"2023-08-15T08:14:39.135696Z","shell.execute_reply.started":"2023-08-15T08:14:28.707439Z","shell.execute_reply":"2023-08-15T08:14:39.134397Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1TNDLpESESQSVmWXKS-EebZUl2CKNH6a2\nTo: /kaggle/working/coc_tiny2.pth.tar\n100%|██████████████████████████████████████| 67.2M/67.2M [00:02<00:00, 24.7MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"model = coc_tiny2(num_classes=2).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:45:38.943943Z","iopub.execute_input":"2023-08-15T09:45:38.945095Z","iopub.status.idle":"2023-08-15T09:45:39.078075Z","shell.execute_reply.started":"2023-08-15T09:45:38.945037Z","shell.execute_reply":"2023-08-15T09:45:39.077163Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# model.load_state_dict(torch.load(\"/kaggle/working/coc_tiny2.pth.tar\")['state_dict'])","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:42:59.506829Z","iopub.execute_input":"2023-08-15T09:42:59.507888Z","iopub.status.idle":"2023-08-15T09:42:59.676079Z","shell.execute_reply.started":"2023-08-15T09:42:59.507850Z","shell.execute_reply":"2023-08-15T09:42:59.674997Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nloss_function = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:45:40.903241Z","iopub.execute_input":"2023-08-15T09:45:40.903640Z","iopub.status.idle":"2023-08-15T09:45:40.911993Z","shell.execute_reply.started":"2023-08-15T09:45:40.903612Z","shell.execute_reply":"2023-08-15T09:45:40.910912Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"model, optimizer, _ = training_loop(model, loss_function, optimizer, train_data_loader, test_data_loader, 25, device)","metadata":{"execution":{"iopub.status.busy":"2023-08-15T09:45:43.059013Z","iopub.execute_input":"2023-08-15T09:45:43.060081Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epochs: 0, Train_loss: 0.6943733618736267, Valid_loss: 0.6949191670508901, Train_accuracy: 50.544999999999995, Valid_accuracy: 50.42\nEpochs: 1, Train_loss: 0.6823418574333191, Valid_loss: 0.6793220381068575, Train_accuracy: 55.57, Valid_accuracy: 55.14\nEpochs: 2, Train_loss: 0.6643706129074096, Valid_loss: 0.6471789707044128, Train_accuracy: 62.19, Valid_accuracy: 61.44\nEpochs: 3, Train_loss: 0.6276456370830535, Valid_loss: 0.6146583657735473, Train_accuracy: 66.145, Valid_accuracy: 65.62\nEpochs: 4, Train_loss: 0.6012216151714325, Valid_loss: 0.6140685962263945, Train_accuracy: 67.05499999999999, Valid_accuracy: 66.14\n","output_type":"stream"}]}]}